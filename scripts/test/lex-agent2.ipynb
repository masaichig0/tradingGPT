{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9451f829",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create `OpenAIEmbeddings` model\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "embeddings = OpenAIEmbeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "474ace58",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Masa\\anaconda3\\lib\\site-packages\\pinecone\\index.py:4: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'dimension': 1536,\n",
       " 'index_fullness': 0.0,\n",
       " 'namespaces': {'': {'vector_count': 12505}},\n",
       " 'total_vector_count': 12505}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pinecone \n",
    "import os\n",
    "from langchain.vectorstores import Pinecone\n",
    "from tqdm.autonotebook import tqdm\n",
    "\n",
    "# initialize pinecone\n",
    "pinecone.init(\n",
    "    api_key=os.environ.get(\"PINECONE_API_KEY\"),  # find at app.pinecone.io\n",
    "    environment= \"us-east4-gcp\"  # next to api key in console\n",
    ")\n",
    "\n",
    "index_name = \"index-name\"\n",
    "index = pinecone.Index(index_name)\n",
    "index_stats_response = index.describe_index_stats()\n",
    "index_stats_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1e6ac368",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore = Pinecone(index=index, embedding_function=embeddings.embed_query, text_key='text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0709d945",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import OpenAI\n",
    "from langchain.chains.summarize import load_summarize_chain\n",
    "\n",
    "llm = OpenAI(temperature=0)\n",
    "summary_chain = load_summarize_chain(llm, chain_type=\"map_reduce\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9e9c4a39",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import AnalyzeDocumentChain\n",
    "summarize_document_chain = AnalyzeDocumentChain(combine_docs_chain=summary_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cc060b78",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "argument of type 'Pinecone' is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [6]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43msummarize_document_chain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvectorstore\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\langchain\\chains\\base.py:213\u001b[0m, in \u001b[0;36mChain.run\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    211\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    212\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`run` supports only one positional argument.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 213\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_keys[\u001b[38;5;241m0\u001b[39m]]\n\u001b[0;32m    215\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kwargs \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m args:\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m(kwargs)[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_keys[\u001b[38;5;241m0\u001b[39m]]\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\langchain\\chains\\base.py:116\u001b[0m, in \u001b[0;36mChain.__call__\u001b[1;34m(self, inputs, return_only_outputs)\u001b[0m\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m, \u001b[38;5;167;01mException\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    115\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose)\n\u001b[1;32m--> 116\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m    117\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_manager\u001b[38;5;241m.\u001b[39mon_chain_end(outputs, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose)\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprep_outputs(inputs, outputs, return_only_outputs)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\langchain\\chains\\base.py:113\u001b[0m, in \u001b[0;36mChain.__call__\u001b[1;34m(self, inputs, return_only_outputs)\u001b[0m\n\u001b[0;32m    107\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_manager\u001b[38;5;241m.\u001b[39mon_chain_start(\n\u001b[0;32m    108\u001b[0m     {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m},\n\u001b[0;32m    109\u001b[0m     inputs,\n\u001b[0;32m    110\u001b[0m     verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose,\n\u001b[0;32m    111\u001b[0m )\n\u001b[0;32m    112\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 113\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m, \u001b[38;5;167;01mException\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    115\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\langchain\\chains\\combine_documents\\base.py:113\u001b[0m, in \u001b[0;36mAnalyzeDocumentChain._call\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m    111\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_call\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs: Dict[\u001b[38;5;28mstr\u001b[39m, Any]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Dict[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mstr\u001b[39m]:\n\u001b[0;32m    112\u001b[0m     document \u001b[38;5;241m=\u001b[39m inputs[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_key]\n\u001b[1;32m--> 113\u001b[0m     docs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtext_splitter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_documents\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mdocument\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    114\u001b[0m     \u001b[38;5;66;03m# Other keys are assumed to be needed for LLM prediction\u001b[39;00m\n\u001b[0;32m    115\u001b[0m     other_keys \u001b[38;5;241m=\u001b[39m {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m inputs\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_key}\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\langchain\\text_splitter.py:56\u001b[0m, in \u001b[0;36mTextSplitter.create_documents\u001b[1;34m(self, texts, metadatas)\u001b[0m\n\u001b[0;32m     54\u001b[0m documents \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, text \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(texts):\n\u001b[1;32m---> 56\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m     57\u001b[0m         new_doc \u001b[38;5;241m=\u001b[39m Document(\n\u001b[0;32m     58\u001b[0m             page_content\u001b[38;5;241m=\u001b[39mchunk, metadata\u001b[38;5;241m=\u001b[39mcopy\u001b[38;5;241m.\u001b[39mdeepcopy(_metadatas[i])\n\u001b[0;32m     59\u001b[0m         )\n\u001b[0;32m     60\u001b[0m         documents\u001b[38;5;241m.\u001b[39mappend(new_doc)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\langchain\\text_splitter.py:277\u001b[0m, in \u001b[0;36mRecursiveCharacterTextSplitter.split_text\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m    275\u001b[0m     separator \u001b[38;5;241m=\u001b[39m _s\n\u001b[0;32m    276\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m--> 277\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43m_s\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtext\u001b[49m:\n\u001b[0;32m    278\u001b[0m     separator \u001b[38;5;241m=\u001b[39m _s\n\u001b[0;32m    279\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[1;31mTypeError\u001b[0m: argument of type 'Pinecone' is not iterable"
     ]
    }
   ],
   "source": [
    "summarize_document_chain.run(vectorstore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b0c5be15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create prompt template with PromprTemplate\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.memory import ConversationEntityMemory\n",
    "\n",
    "chat = ChatOpenAI(temperature=0, model_name=\"gpt-3.5-turbo\")\n",
    "memory = ConversationEntityMemory(llm=chat)\n",
    "cd_template = \"\"\"Given the following conversation and a follow up question, rephras the follow up question to be a standalone\n",
    "question. \n",
    "\n",
    "Chat History:\n",
    "{chat_history}\n",
    "Follow Up Input: {question}\n",
    "standalone question:\"\"\"\n",
    "CONDENSE_PROMPT = PromptTemplate(template=cd_template, \n",
    "                                           input_variables=[\"chat_history\", \"question\"])\n",
    "\n",
    "qa_prompt = \"\"\"You are AI Entertainer and brilliant in economics and finance. Please use the following pieces of context \n",
    "to answerthe question at the end. You must make some kind of conclusion and DO NOT give me a answer that YOU DON'T KNOW. \n",
    "Your answer is NOT the advise and the porpose is for your opinion is entertainment use.\n",
    "\n",
    "{context}\n",
    "Question: {question}\n",
    "Your Answer: \n",
    "\"\"\"\n",
    "\n",
    "QA_PROMPT = PromptTemplate(template=qa_prompt, \n",
    "                                    input_variables=['context', 'question'])\n",
    "\n",
    "# Create llm chain\n",
    "\n",
    "chain = ConversationalRetrievalChain.from_llm(chat, retriever=vectorstore.as_retriever(),\n",
    "                                            condense_question_prompt=CONDENSE_PROMPT,\n",
    "                                            qa_prompt=QA_PROMPT,\n",
    "                                            #memory=memory, \n",
    "                                             return_source_documents=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e803a8ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chat with News\n",
    "chat_history = []\n",
    "question = \"What do you think about the future of AI?\"\n",
    "result1 = chain({\"question\": question, \"chat_history\": chat_history})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "63cd699e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'What do you think about the future of AI?',\n",
       " 'chat_history': [],\n",
       " 'answer': \"I believe that AI will continue to advance and become faster, cheaper, and more general in its applications. However, there are also concerns about the potential negative effects of AI, such as privacy and bias in data. I think it's important for there to be more discourse between government and experts on these topics. Additionally, I see recommendation systems as having the biggest impact on our society and believe that engineers need to think deeply about their societal implications. In terms of learning about AI, I recommend various courses and tutorials, as well as books such as the deep learning book by Yoshua Bengio and Ian Goodfellow. Overall, I am optimistic about the potential of AI, but also recognize the need for caution and ethical considerations.\",\n",
       " 'source_documents': [Document(page_content=\"So I think I see exciting, exciting or not so exciting, but not harmful futures. I think it's very difficult to create AI systems that will kill people, that aren't like literally weapons of war. They're like, it'll always be people killing people. Like the things we should be worried about is other people. That's the, that's the fundamental. So there's a lot of ways, you know, nuclear weapons. There's a lot of existential threats to our society that are fundamentally human at the core. And AI will be, might be tools of that, but there'll be also tools to defend against that. I also see AI proliferating as companions. I think companionship will be a really interesting, like, we will more and more live as we already do in the digital world. Like you have an identity on Twitter, Instagram, especially if it's anonymous or something. You have this identity you've created and that will continue growing more and more, especially for people born now, that it's kind of this artificial identity that we live much more in the digital space. And in that digital space, as opposed to the physical space is where AI can thrive much more currently. It'll thrive there first. And so we'll live in a world with a lot of intelligent first assistants, but also just intelligent agents. And I do believe they should have rights. And in this contentious time of people, groups fighting for rights, I feel really bad saying they should have equal rights, but I believe that. I've talked to, if you read the work of Peter Singer, of looking, like my favorite food is steak. I love meat, but I also feel horrible about the torture of animals. And that's the same kind of, to me, the way our society thinks about animals is a very similar way we should be thinking about robots, or we will be thinking about robots in, I would say about 20 years. One final question. Will they become our masters? No, they will not be our masters. What I'm really worried about is what we're going to become our masters are owners of large tech companies who use these tools to control human beings, first unintentionally and then intentionally. So we need to make sure that we democratize AI. It's the same kind of thing that we did with government. We make sure that we, at the heads of tech companies, if maybe people in this room will be heads of tech companies one day, we have people like George Washington who relinquished power at the founding of this country. Forget all the other horrible things he did, but he relinquished power as opposed to Stalin and all the other horrible human beings who have sought instead absolute power, which will be the 21st century. AI will be the tools of power in the hands of 25-year-old nerds. We should be very careful about that future. So the humans will become our masters, not the AI. AI will save us. So on that note, thank you very much.\", metadata={'channel_id': 'UCSHZKyawb77ixDdsGog4iWA', 'chunk': 19.0, 'published': datetime.datetime(2020, 1, 10, 16, 4, 31), 'source': 'https://youtu.be/0VH1Lim8gL8', 'title': 'Deep Learning State of the Art (2020)', 'video_id': '0VH1Lim8gL8'}),\n",
       "  Document(page_content=\"And if he says it's impossible, he's almost surely wrong. But you don't know what the timescale is. The timescale is critical, right. So, what are your thoughts on this new summer of AI now in the work with machine learning and neural networks? You've kind of mentioned that you started to try to explore and look into this world that seems fundamentally different from the world of heuristics and algorithms like search, that it's now purely sort of trying to take huge amounts of data and learn from that data, right? Programs from the data. Yeah. Look, I think it's very interesting. I am incredibly far from an expert. Most of what I know I've learned from my students, and they're probably disappointed in how little I've learned from them. But I think it has I think it has tremendous potential for certain kinds of things. I mean, games is one where it obviously has had an effect on some of the others as well. I think there's, and this is speaking from definitely not expertise, I think there are serious problems in certain kinds of machine learning, at least because what they're learning from is the data that we give them. And if the data we give them has something wrong with it, then what they learn from it is probably wrong, too. And the obvious thing is some kind of bias in the data, that the data has stuff in it, like, I don't know, women aren't as good as men at something, okay? That's just flat wrong. But if it's in the data because of historical treatment, then that machine learning stuff will propagate that. And that is a serious worry. The positive part of that is what machine learning does is reveal the bias in the data and puts a mirror to our own society. And in so doing, helps us remove the bias, you know, helps us work on ourselves, puts a mirror to ourselves. Yeah, that's an optimistic point of view. And if it works that way, that would be absolutely great. And what I don't know is whether it does work that way, or whether the AI mechanisms or machine learning mechanisms reinforce and amplify things that have been wrong in the past. And I don't know, but I think that's a serious thing that we have to be concerned about. Let me ask you an out there question. Okay, I know nobody knows, but what do you think it takes to build a system of human level intelligence? That's been the dream from the 60s. We talk about games, about language, about image recognition, but really the dream is to create human level, or superhuman level intelligence. What do you think it takes to do that? And are we close? I haven't a clue and I don't know, roughly speaking. I mean, this was... I was trying to trick you into hypothesizing. Yeah, I mean, Turing talked about this in his paper on machine intelligence back in, geez, I don't know, early 50s or something like that. And he had the idea of the Turing test. And I don't know whether the Turing test is... Is a good test of intelligence. I don't know. It's an interesting test. At least it's in some vague sense objective, whether you can read anything into the conclusions is a different story. Do you have worries, concerns, excitement about the future of artificial intelligence? So there's a lot of people who are worried. And you can speak broadly than just artificial intelligence. It's basically computing taking over the world in various forms. Are you excited by this future, this possibility of computing being everywhere? Or are you worried? It's some combination of those. I think almost all technologies over the long run are for good, but there's plenty of examples where they haven't been good, either over a long run for some people or over a short run. And computing is one of those and AI within it is going to be one of those as well. But computing broadly, I mean, for just a today example is privacy, that the use of things like social media and so on means that... And the commercial surveillance means that there's an enormous amount more known about us by people, other businesses, government, whatever, than perhaps one ought to feel comfortable with. So that's an example. So that's an example of a possible negative effect of computing being everywhere. It's an interesting one because it could also be a positive if leveraged correctly. There's a big if there. So I have a deep interest in human psychology and humans seem to be very paranoid about this data thing, but that varies depending on age group\", metadata={'channel_id': 'UCSHZKyawb77ixDdsGog4iWA', 'chunk': 22.0, 'published': datetime.datetime(2020, 7, 18, 19, 24, 51), 'source': 'https://youtu.be/O9upVbGSBFo', 'title': 'Brian Kernighan: UNIX, C, AWK, AMPL, and Go Programming | Lex Fridman Podcast #109', 'video_id': 'O9upVbGSBFo'}),\n",
       "  Document(page_content=\"So, let me ask you to be arrogant. What do AI systems, with or without physical bodies, look like 100 years from now? If you would just, you can't predict, but if you were to philosophize and imagine, do. Can I first justify the arrogance before you try to push me beyond it? Sure. I mean, there are examples. People figured out how electricity worked. They had no idea that that was going to lead to cell phones, right? I mean, things can move awfully fast once new technologies are perfected. Even when they made transistors, they weren't really thinking that cell phones would lead to social networking. There are, nevertheless, predictions of the future, which are statistically unlikely to come to be, but nevertheless is the best. You're asking me to be wrong. Asking you to be statistically. In which way would I like to be wrong? Pick the least unlikely to be wrong thing, even though it's most very likely to be wrong. I mean, here's some things that we can safely predict, I suppose. We can predict that AI will be faster than it is now. It will be cheaper than it is now. It will be better in the sense of being more general and applicable in more places. It will be pervasive. I mean, these are easy predictions. I'm sort of modeling them in my head on Jeff Bezos' famous predictions. He says, I can't predict the future, not in every way. I'm paraphrasing. But I can predict that people will never want to pay more money for their stuff. They're never going to want it to take longer to get there. You can't predict everything, but you can predict some things. Sure, of course it's going to be faster and better. We can't really predict the full scope of where AI will be in a certain period. I think it's safe to say that, although I'm very skeptical about current AI, that it's possible to do much better. There's no in-principle argument that says AI is an insolvable problem, that there's magic inside our brains that will never be captured. I mean, I've heard people make those kinds of arguments. I don't think they're very good. So AI is going to come, and probably 500 years is plenty to get there. And then once it's here, it really will change everything. So when you say AI is going to come, are you talking about human level intelligence? So maybe I like the term general intelligence. So I don't think that the ultimate AI, if there is such a thing, is going to look just like humans. I think it's going to do some things that humans do better than current machines, like reason flexibly and understand language and so forth. But that doesn't mean they have to be identical to humans. So for example, humans have terrible memory, and they suffer from what some people call motivated reasoning. So they like arguments that seem to support them, and they dismiss arguments that they don't like. There's no reason that a machine should ever do that. Lex Dressler So you see that those limitations of memory as a bug, not a feature. David Absolutely. I'll say two things about that. One is I was on a panel with Danny Kahneman, the Nobel Prize winner, last night, and we were talking about this stuff. And I think what we converged on is that humans are a low bar to exceed. They may be outside of our skill right now as AI programmers, but eventually AI will exceed it. So we're not talking about human level AI. We're talking about general intelligence that can do all kinds of different things and do it without some of the flaws that human beings have. The other thing I'll say is I wrote a whole book, actually, about the flaws of humans. It's actually a nice bookend to the, or counterpoint to the current book. So I wrote a book called Kluge, which was about the limits of the human mind. The current book is kind of about those few things that humans do a lot better than machines. Lex Dressler Do you think it's possible that the flaws of the human mind, the limits of memory, our mortality, our bias is a strength, not a weakness? That is the thing that enables from which motivation springs and meaning springs? I've heard a lot of arguments like this. I've never found them that convincing. I think that there's a lot of making lemonade out of lemons. So we, for example, do a lot of free association where one idea just leads to the next and they're not really that well connected. And we enjoy that and we make poetry out of it. And we make kind of movies with free associations and it's fun and whatever. I don't think that's really a virtue of the system\", metadata={'channel_id': 'UCSHZKyawb77ixDdsGog4iWA', 'chunk': 5.0, 'published': datetime.datetime(2019, 10, 3, 11, 31, 2), 'source': 'https://youtu.be/vNOTDn3D_RI', 'title': 'Gary Marcus: Toward a Hybrid of Deep Learning and Symbolic AI | Lex Fridman Podcast #43', 'video_id': 'vNOTDn3D_RI'}),\n",
       "  Document(page_content=\"So my hope in this, in the politics space, in the public discourse space for 2020 is less fear of AI and more discourse between government and experts on topics of privacy, cybersecurity, and so on. And then transparency and recommender systems. I think the most exciting, the most powerful artificial intelligence system space for the next couple of decades is recommendation systems. Very little talked about it seems like, but they're going to have the biggest impact on our society because they affect how the information we see, how we learn, what we think, how we communicate. These algorithms are controlling us. And we have to really think deeply as engineers of how to speak up and think about their societal implications, not just in terms of bias and so on, which are sort of ethical considerations, which are really important, but stuff that's like the elephant in the room that's hidden, which is controlling how we think, how we see the world, the moral system under which we operate. Quickly to mention and wrapping up with a few minutes of questions, if there are any, is the deep learning courses this year, before the last few years has been a lot of incredible courses on deep learning, on reinforcement learning. What I would very much recommend for people is the fast AI course from Jeremy Howard, which uses their wrapper around PyTorch. It's to me, the best introduction to deep learning for people who are here or might be listening elsewhere, are thinking about learning more about deep learning. That is to me, the best course. Also paid, but everyone loves Andrew Yang, is the deep learning AI Coursera course on deep learning is excellent, especially for beginners. And then Stanford has two courses, two excellent courses on visual recognition. So convolution neural nets, originally taught by Andrew Karpathy and natural language processing, excellent courses. And of course, here at MIT, there's a bunch of courses, especially on the fundamentals, on the mathematics, on linear algebra, statistics. And I have a few lectures up online that you should never watch. Then on the reinforcement learning side, David Silver is one of the greatest people in understanding reinforcement learning from DeepMind. He has a great course, Introduction to Reinforcement Learning, Spinning Up and Deep Reinforcement Learning from OpenAI, I highly recommend. Here, just for the slides that I'll share online, there's a lot of tutorials. One of my favorite lists of tutorials, which is, I believe, the best way to learn machine learning, deep learning, natural language processing in general, is just code. Just build it yourself, build the models oftentimes from scratch. Here's a list of tutorials with that link, over 200 tutorials on topics from deep RL to optimization to back prop, LSTMs, convolutional recurrent neural networks, everything. Over 200 of the best machine learning NLP and Python tutorials by Robbie Allen. You can Google that, or you can click the link. I love it, highly recommend. The three books I would recommend, of course, the deep learning book by Yoshua Bengio and Ian Goodfellow and Aaron Corville. That's more sort of the fundamental thinking about from philosophy to the specific techniques of the deep learning and the practical grok in deep learning, which Andrew Trask will be here Wednesday. His book, Grok in Deep Learning, I think is the best for beginners book on deep learning. I love it. He implements everything from scratch. It's extremely accessible. 2019, I think it was published, maybe 18, but I love it. And then Francois Chollet, the best book on Keras and TensorFlow and really deep learning as well as deep learning with Python, although you shouldn't buy it, I think, because he is supposed to come up with version two, which I think will cover TensorFlow 2.0. It'll be an excellent book. And when he's here Monday, you should torture him and tell him to finish writing. He was supposed to finish writing in 2019. Okay. My general hopes, as I mentioned, for 2020 is I'd love to see common sense reasoning, not necessarily enter the world of deep learning, but be a part of artificial intelligence and the problems that people tackle. As I've been harboring, active learning is to me is the most important aspect of real world application of deep learning. There's not enough research. There should be way more research. I'd love to see active learning, lifelong learning. That's what we all do as human beings. That's what AI systems need to do. Continually learn from their mistakes over time. Start out dumb, become brilliant over time. Open domain conversation with the Alexa prize. I would love to see breakthroughs there. Alexa folks thinks we're still two or three decades away, but that's what everybody says before the breakthrough\", metadata={'channel_id': 'UCSHZKyawb77ixDdsGog4iWA', 'chunk': 15.0, 'published': datetime.datetime(2020, 1, 10, 16, 4, 31), 'source': 'https://youtu.be/0VH1Lim8gL8', 'title': 'Deep Learning State of the Art (2020)', 'video_id': '0VH1Lim8gL8'})]}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b2abc192",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat1 = result1[\"question\"], result1['answer']\n",
    "chat_history.append(chat1)\n",
    "question = \"Please tell me more about potential negative effects of AI.\"\n",
    "result1 = chain({\"question\": question, \"chat_history\": chat_history})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3f203d52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'Please tell me more about potential negative effects of AI.',\n",
       " 'chat_history': [('What do you think about the future of AI?',\n",
       "   \"I believe that AI will continue to advance and become faster, cheaper, and more general in its applications. However, there are also concerns about the potential negative effects of AI, such as privacy and bias in data. I think it's important for there to be more discourse between government and experts on these topics. Additionally, I see recommendation systems as having the biggest impact on our society and believe that engineers need to think deeply about their societal implications. In terms of learning about AI, I recommend various courses and tutorials, as well as books such as the deep learning book by Yoshua Bengio and Ian Goodfellow. Overall, I am optimistic about the potential of AI, but also recognize the need for caution and ethical considerations.\")],\n",
       " 'answer': 'One potential negative effect of AI is the propagation of bias in machine learning if the data used to train the AI has biases. This can lead to incorrect conclusions and reinforce harmful stereotypes. Additionally, there is a concern about the use of AI in autonomous weapons systems, which could lead to unintended consequences and escalation of violence. Another concern is the potential for large tech companies to use AI to control human behavior, which could lead to a loss of privacy and individual autonomy. However, it is important to note that every technology has both positive and negative consequences, and it is up to society to ensure that AI is used for the greater good.',\n",
       " 'source_documents': [Document(page_content=\"And if he says it's impossible, he's almost surely wrong. But you don't know what the timescale is. The timescale is critical, right. So, what are your thoughts on this new summer of AI now in the work with machine learning and neural networks? You've kind of mentioned that you started to try to explore and look into this world that seems fundamentally different from the world of heuristics and algorithms like search, that it's now purely sort of trying to take huge amounts of data and learn from that data, right? Programs from the data. Yeah. Look, I think it's very interesting. I am incredibly far from an expert. Most of what I know I've learned from my students, and they're probably disappointed in how little I've learned from them. But I think it has I think it has tremendous potential for certain kinds of things. I mean, games is one where it obviously has had an effect on some of the others as well. I think there's, and this is speaking from definitely not expertise, I think there are serious problems in certain kinds of machine learning, at least because what they're learning from is the data that we give them. And if the data we give them has something wrong with it, then what they learn from it is probably wrong, too. And the obvious thing is some kind of bias in the data, that the data has stuff in it, like, I don't know, women aren't as good as men at something, okay? That's just flat wrong. But if it's in the data because of historical treatment, then that machine learning stuff will propagate that. And that is a serious worry. The positive part of that is what machine learning does is reveal the bias in the data and puts a mirror to our own society. And in so doing, helps us remove the bias, you know, helps us work on ourselves, puts a mirror to ourselves. Yeah, that's an optimistic point of view. And if it works that way, that would be absolutely great. And what I don't know is whether it does work that way, or whether the AI mechanisms or machine learning mechanisms reinforce and amplify things that have been wrong in the past. And I don't know, but I think that's a serious thing that we have to be concerned about. Let me ask you an out there question. Okay, I know nobody knows, but what do you think it takes to build a system of human level intelligence? That's been the dream from the 60s. We talk about games, about language, about image recognition, but really the dream is to create human level, or superhuman level intelligence. What do you think it takes to do that? And are we close? I haven't a clue and I don't know, roughly speaking. I mean, this was... I was trying to trick you into hypothesizing. Yeah, I mean, Turing talked about this in his paper on machine intelligence back in, geez, I don't know, early 50s or something like that. And he had the idea of the Turing test. And I don't know whether the Turing test is... Is a good test of intelligence. I don't know. It's an interesting test. At least it's in some vague sense objective, whether you can read anything into the conclusions is a different story. Do you have worries, concerns, excitement about the future of artificial intelligence? So there's a lot of people who are worried. And you can speak broadly than just artificial intelligence. It's basically computing taking over the world in various forms. Are you excited by this future, this possibility of computing being everywhere? Or are you worried? It's some combination of those. I think almost all technologies over the long run are for good, but there's plenty of examples where they haven't been good, either over a long run for some people or over a short run. And computing is one of those and AI within it is going to be one of those as well. But computing broadly, I mean, for just a today example is privacy, that the use of things like social media and so on means that... And the commercial surveillance means that there's an enormous amount more known about us by people, other businesses, government, whatever, than perhaps one ought to feel comfortable with. So that's an example. So that's an example of a possible negative effect of computing being everywhere. It's an interesting one because it could also be a positive if leveraged correctly. There's a big if there. So I have a deep interest in human psychology and humans seem to be very paranoid about this data thing, but that varies depending on age group\", metadata={'channel_id': 'UCSHZKyawb77ixDdsGog4iWA', 'chunk': 22.0, 'published': datetime.datetime(2020, 7, 18, 19, 24, 51), 'source': 'https://youtu.be/O9upVbGSBFo', 'title': 'Brian Kernighan: UNIX, C, AWK, AMPL, and Go Programming | Lex Fridman Podcast #109', 'video_id': 'O9upVbGSBFo'}),\n",
       "  Document(page_content=\"So if AI take that in consideration in order to protect us against ourself, it could hurt us. I don't know if you understand what I mean. What do you think about that? Does it make you afraid sometimes? Not because of AI, but because of what humans are doing that AI could do to us to prevent us of hurting ourselves? Yeah, I mean, definitely it can bring out the worst in human nature and provide tools for evil people to do evil things at a larger scale. But I just think it depends what you think human beings are. I tend to believe that as we get more intelligent, we start to see the value, the evolutionary value and the value in terms of happiness of being good to each other. And I think AI, if you look at AI as an optimization problem of how to create a civilization that works well and expands throughout the universe, I think love is much more effective. So AI will help us maximize that. I think there's going to be always spikes throughout as it has been through human history where charismatic leaders will do evil onto the world in the name of good. You have the Stalin and the Hitlers and all of that. But ultimately, over time, I think technology will give the good people power and the evil people less power. Now there's a lot of ways that that won't be the case. There's a lot of ways for it to go wrong. And Elon talks about them. But I honestly think in terms of intelligent AI, that's going to bring more love to the world. The thing I'm concerned about is dumb AI. So there's been a lot of discussion between China and the United States recently on autonomous weapons system. This is something people don't, they're afraid to talk about, but there's now a race where the United States has officially said that they're not against adding AI to its weapon systems. So now the US military is adding automation, adding intelligence to its drones, to its anything that can create damage. And so of course, and they did this so in response to China doing that. So you can imagine this is Terminator. You think about Terminator's intelligent systems, they're not, they're pretty dumb. The point is they're efficient at doing what they do. And in the space of war, efficient at doing what you do means killing. So that I'm really afraid of, but those are dumb AI. Those aren't your loving, deep, fulfilling relationships. It's like efficiently being able to fly, to plan the trajectory of dropping bombs, of missiles, of how to do counter attacks, of how to maximize the destruction of a particular facility instead of individuals. And then that can just escalate. And as opposed to the cold war with the Soviet Union, this could be a hot war. And then the consequences, once you allow, it's kind of terrifying because currently the drones are operated by humans. So you have, say you have information about, intelligence gives you information about a particular terrorist located in this area. And then you use drones to maybe, the automation there is to help you figure out what is the best trajectory to strike at that location. So you still have a human that pulls the trigger at the end, dropping the bomb. Now automation and AI and autonomous weapons systems might be where you say, there's a bad guy over here. You figure out how to get rid of the bad guy. So then of course the systems will be very good at finding the right trajectory and so on, but there's bugs that can happen, unexpected bugs that the system might figure out that there is this bad guy might actually be in these other five locations. So it might make sense to cover the entire area. And so you might drop bombs on the entire area and then that's just, okay, so that's going to lead to a lot of destruction at the scale of a city, but then you can immediately take that to nuclear weapons. If you add automation to responding to counterattacks to nuclear weapons, somebody, you might get information that somebody is planning a nuclear attack on the United States and the AI system will immediately respond. And it can respond to the scale of launching nuclear weapons itself. And so there's all of these possibilities that don't require much intelligence. And that's exceptionally concerning. I'm like you, I do not believe there is babies that are born bad. I think people do bad things because of their experience. However, if I look through my experience and from what I can see is some very often men's of power wants more power. That's what makes me afraid with... Yeah, no, absolutely. Listen, I've been, I've come from the Soviet Union. Stalin is arguably one of the most powerful humans in history. He's not talked to often enough about by the evils he's done\", metadata={'channel_id': 'UCSHZKyawb77ixDdsGog4iWA', 'chunk': 22.0, 'published': datetime.datetime(2021, 4, 26, 3, 34, 31), 'source': 'https://youtu.be/fIPxfzfOTxk', 'title': 'Georges St-Pierre: The Science of Fighting | Lex Fridman Podcast #179', 'video_id': 'fIPxfzfOTxk'}),\n",
       "  Document(page_content=\"So I think I see exciting, exciting or not so exciting, but not harmful futures. I think it's very difficult to create AI systems that will kill people, that aren't like literally weapons of war. They're like, it'll always be people killing people. Like the things we should be worried about is other people. That's the, that's the fundamental. So there's a lot of ways, you know, nuclear weapons. There's a lot of existential threats to our society that are fundamentally human at the core. And AI will be, might be tools of that, but there'll be also tools to defend against that. I also see AI proliferating as companions. I think companionship will be a really interesting, like, we will more and more live as we already do in the digital world. Like you have an identity on Twitter, Instagram, especially if it's anonymous or something. You have this identity you've created and that will continue growing more and more, especially for people born now, that it's kind of this artificial identity that we live much more in the digital space. And in that digital space, as opposed to the physical space is where AI can thrive much more currently. It'll thrive there first. And so we'll live in a world with a lot of intelligent first assistants, but also just intelligent agents. And I do believe they should have rights. And in this contentious time of people, groups fighting for rights, I feel really bad saying they should have equal rights, but I believe that. I've talked to, if you read the work of Peter Singer, of looking, like my favorite food is steak. I love meat, but I also feel horrible about the torture of animals. And that's the same kind of, to me, the way our society thinks about animals is a very similar way we should be thinking about robots, or we will be thinking about robots in, I would say about 20 years. One final question. Will they become our masters? No, they will not be our masters. What I'm really worried about is what we're going to become our masters are owners of large tech companies who use these tools to control human beings, first unintentionally and then intentionally. So we need to make sure that we democratize AI. It's the same kind of thing that we did with government. We make sure that we, at the heads of tech companies, if maybe people in this room will be heads of tech companies one day, we have people like George Washington who relinquished power at the founding of this country. Forget all the other horrible things he did, but he relinquished power as opposed to Stalin and all the other horrible human beings who have sought instead absolute power, which will be the 21st century. AI will be the tools of power in the hands of 25-year-old nerds. We should be very careful about that future. So the humans will become our masters, not the AI. AI will save us. So on that note, thank you very much.\", metadata={'channel_id': 'UCSHZKyawb77ixDdsGog4iWA', 'chunk': 19.0, 'published': datetime.datetime(2020, 1, 10, 16, 4, 31), 'source': 'https://youtu.be/0VH1Lim8gL8', 'title': 'Deep Learning State of the Art (2020)', 'video_id': '0VH1Lim8gL8'}),\n",
       "  Document(page_content=\"And particularly as life expectancies get longer and so forth, there's just a lot more need for a greater percentage of the population to kind of just be serviced with a high level of efficiency because otherwise we're gonna have a really hard time kind of scaling to what's ahead in the next 50 years in front of us. Yeah, and you're absolutely right. Every technology has negative consequences to positive consequences. We tend to focus on the negative a little bit too much. In fact, autonomous trucks are often brought up as an example of artificial intelligence and robots in general taking our jobs. And as we've talked about briefly here, we talk a lot with Steve, you know, that it is a concern that automation will take away certain jobs, it'll create other jobs. So there's temporary pain, hopefully temporary, but pain is pain and people suffer. And that human suffering is really important to think about how, but trucking is, I mean, there's a lot written on this, is I would say far from the thing that will cause the most pain. Yeah, there's even more positive properties about trucking where not only is there just a huge shortage which is gonna increase, the average age of truck drivers is getting closer to 50 because the younger people aren't wanting to come into it. They're trying to like incentivize, lower the age limit, like all these sorts of things. And the demand is just gonna increase. And the least favorable, like, I mean, it depends on the person but in most cases, the least favorable types of routes are the massive long haul routes where you're on the road away from your family 300 plus days a year. Steve's talked about the pain of those kinds of routes from a family perspective. You're basically away from family. It's not just hours, work insane hours but it's also just time away from family. Obesity rate is through the roof because you're just sitting all day. Like it's really, really tough. And that's also where like the biggest kind of safety risk is because of fatigue. And so when you think of the gradual evolution of how trucking comes in, first of all, it's not overnight. It's gonna take decades to kind of phase in all the, like there's just a long, long, long road ahead. But the routes and the portions of trucking that are gonna require humans the longest and benefit the most from humans are the short haul and most complicated kind of more urban routes which are also the more pleasant ones, which are less continual driving time, more flexibility on like geography and location and you get to kind of sleep at your own home. And so- And very importantly, if you optimize the logistics, you're going to use humans much better. And thereby pay them much better because like one of the biggest problems is truck drivers currently are paid by like how much they drive. So they really feel the pain of inefficient logistics because like if they're just sitting around for hours, which they often do not driving, waiting, they're not getting paid for that time. And that, so like logistics has a significant impact on the quality of life of a truck driver. And a high percentage of trucks are like empty because of inefficiencies in the system. Yeah, it's one of those things where like, and the other thing is when you increase the efficiency of a system like this, the overall net like volume of the system tends to increase, right? Like the entire market cap of trucking is going to go up when the efficiency improves and facilitates both growth in industries and better utilization of trucking. And so that on its own just creates more and more demand which of all the places where AI comes in and starts to really kind of reshape an industry. This is one of those where like, there's just a lot of positives that for at least any time in the foreseeable future seem really lined up in a good way to kind of come in and help with the shortage and start to kind of optimize for the routes that are most dangerous and most painful. Yeah, so this is true for trucking, but if we zoom out broader, automation and AI does technology broadly, I would say, but automation is a thing that has a potential in the next couple of decades to shift the kind of jobs available to humans. And so that results in, like I said, human suffering because people lose their jobs, there's economic pain there, and there's also a pain of meaning. So for a lot of people, work is a source of meaning, it's a source of identity, of pride, of pride in getting good at the job, pride in craftsmanship and excellence, which is what truck drivers talk about. But this is true for a lot of jobs\", metadata={'channel_id': 'UCSHZKyawb77ixDdsGog4iWA', 'chunk': 39.0, 'published': datetime.datetime(2021, 11, 16, 23, 18, 44), 'source': 'https://youtu.be/U_AREIyd0Fc', 'title': 'Boris Sofman: Waymo, Cozmo, Self-Driving Cars, and the Future of Robotics | Lex Fridman Podcast #241', 'video_id': 'U_AREIyd0Fc'})]}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2cde75dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "question = \"Will AI help us forcast better?\"\n",
    "result1 = chain({\"question\": question, \"chat_history\": chat_history})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "21b881d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'Will AI help us forcast better?',\n",
       " 'chat_history': [('What do you think about the future of AI?',\n",
       "   \"I believe that AI will continue to advance and become faster, cheaper, and more general in its applications. However, there are also concerns about the potential negative effects of AI, such as privacy and bias in data. I think it's important for there to be more discourse between government and experts on these topics. Additionally, I see recommendation systems as having the biggest impact on our society and believe that engineers need to think deeply about their societal implications. In terms of learning about AI, I recommend various courses and tutorials, as well as books such as the deep learning book by Yoshua Bengio and Ian Goodfellow. Overall, I am optimistic about the potential of AI, but also recognize the need for caution and ethical considerations.\")],\n",
       " 'answer': 'AI can be used to improve forecasting by using predictive algorithms and machine learning tools to analyze large amounts of data and identify patterns and trends. This can be applied in various industries such as finance, economics, and weather forecasting. By using AI, we can make more accurate predictions and improve decision-making processes. However, it is important to have a deep understanding of the cause-effect relationships and potential risks associated with relying solely on AI for forecasting. Starting small and gradually integrating AI into the forecasting process can help ensure a successful implementation.',\n",
       " 'source_documents': [Document(page_content=\"We wanna maximize the clones of Chris that can get a lot of likes on Facebook. Okay, just returning briefly to the topic of AI, the topic of AI, are you working on AI stuff, too? A lot of machine learning tools for genomics. For genomics, because I was seeing this interspersed, because you're such a biology, I mean, I suppose computational biology person, but what about the, are you working on age of prediction? Yes, yeah, so you've heard about the book, I guess, yeah. What? That's actually written with the philanthropist I mentioned who we named the fungus after the space station, so that's coming out next year, actually, yeah. What's the effort there? What's your interest in sort of the more narrow AI tools of prediction and machine learning, all that kind of stuff? I think, called The Age of Prediction, so the next book that's coming, is all the ways where machine learning tools, predictive algorithms have fundamentally changed our lives, so some of them are obvious to me, where, for example, when we sequence cancer patients' DNA and we have predictions of exactly which drug will work with it, that's actually a very simple algorithm, but other ones involve predicting, say, the age of blood that's left at the scene of a crime, which uses computational tools to look at each piece of DNA and what it might reveal for its epigenetic state, and then predicting, essentially, how old you are at any given moment. And it also gets to longevity, because sometimes you can see if you're aging faster or slower than you should be, so some tools are in medicine or even forensics, but my favorite part, a lot of the book is, where does this show up in economics as well as in medicine? So predictive tools, I mean, I think the most notorious one people thought about was during the 2012 election and 2016 election especially, we were seeing these really big differences of how Facebook was monitoring feeds. And so prediction is not just better medicine or in finance and economics, people think about stock traders and people doing predictive algorithms, but what you view in your feed, what your vote is and what you saw, Facebook did experiments, they called it social contagion experiments to see can we restructure what people see and then how they respond, actually kind of be really predictive and manipulative, frankly, with what happens, and then can that change how they vote? And the answer seemed to be yes for a good amount of the populace in 2016 in the US. So I think we're seeing more and more of these algorithms show up all over the place, and so the book is about where they're good, for example, in medicine, they're phenomenal, they have fundamentally changed how we treat cancer patients, but where they're risky, like if someone's trying to steal your vote or manipulate your thoughts potentially negatively. So in medicine, you're hopeful about prediction. Yeah, most of the AI in medicine, the machine learning tools for image recognition, for example, for pathology samples, where normally you think, oh, someone takes a big bit of tissue and then puts it onto a slide, normally there's pathologists that have been training for years to look at a chunk of your tissue and say, okay, is this cancer? What kind of cancer? What treatment should I do? But there's an old joke about pathologists where you can give 10 slides to 10 different pathologists and get 11 different diagnoses, which is as awful as it sounds, because you're having someone squint at a stained microscope slide. But instead, if you use a lot of the AI tools where you can actually segment the image, high resolution characterization with multiple probes, it's what AI was built to do. You have a large training data set and then you have test samples afterward, you can do far better than almost every pathologist on the planet and get a much more accurate diagnostic. So that's for breast cancer, for prostate cancer, for leukemia, we've seen the diagnostic tools explode with AI power. Is it currently mostly empowering doctors or can it replace doctors? Watson notoriously was made by IBM to try and replace doctors. I actually was- I love IBM so much. I was in the room when we got a tour of Watson for the first time with the dean of our medical school and these programmers came out and they said, listen, here's this example of a patient and watch Watson diagnose the patient and recommend the right treatment\", metadata={'channel_id': 'UCSHZKyawb77ixDdsGog4iWA', 'chunk': 45.0, 'published': datetime.datetime(2022, 5, 8, 20, 54, 39), 'source': 'https://youtu.be/1C2tPFCGL1U', 'title': 'Chris Mason: Space Travel, Colonization, and Long-Term Survival in Space | Lex Fridman Podcast #283', 'video_id': '1C2tPFCGL1U'}),\n",
       "  Document(page_content=\"Right. Okay. Or it's or. They've happened before. Yeah, they've happened. Just not to you. Let me talk about, if we could, about AI a little bit. So we've Bridgewater Associates manage about 160 billion dollars in assets and artificial intelligence systems algorithms are pretty good with data. What role in the future do you see AI play in analysis and decision making in this kind of data rich and impactful area of investment? I'm going to answer that not only an investment, but I give a more all encompassing rule for AI. As I think, you know, for the last 25 years, we have taken our thinking and put them in algorithms. And so we make decisions that the computer takes those criteria algorithms and they put them there in there and that takes data and they operate as an independent decision maker in parallel with our decision making. So for me, it's like there's a chess game playing and I'm a person with my chess game and I'm saying it made that move and I'm making the move. And how do I compare those two moves? So we've done a lot. But let me give you a rule. If the future can be different from the past. And you don't have deep understanding. You should not rely on AI. OK, those two things, deep understanding of the cause effect relationships that are leading you to place that bet in anything. OK, anything important. Let's say if it was do surgeries and you would say, how do I do surgeries? I think it's totally fine to watch all the doctors do the surgeries. You can put it on. Take a digital camera and do that. Convert that into AI algorithms that go to robots and have them do surgeries. And I'd be comfortable with that because if it'll do that, if it keeps doing the same thing over and over again, and you have enough of that, that would be fine, even though you may not understand the algorithms because you're if the things happening over and over again and you're not asking the future would be the same, that appendicitis or whatever it is will be handled the same way the surgery. That's fine. However, what happens with AI is for the most part is it takes a lot of data and it with a high enough sample size and then it puts together its own algorithms. OK, there are two ways you can come up with algorithms. You can either take your thinking and express them in algorithms or you can say, put the data in and say, what is the algorithm when you that's machine learning? And when you have machine learning, it'll give you equations which quite often are not understandable. If you would try to say, OK, now describe what it's telling you, it's very difficult to describe. And so they can escape understanding. And so it's very good for doing those things that could be done over and over again if you're watching and you're not taking that. But if the future is different from the past and you have that, then you're the future is different from the past and you don't have deep understanding. And if the future is different from the past and you don't have deep understanding, you're going to get in trouble. And so that's the main thing. As far as I is concerned, I and let's say computer replications of thinking in various ways, I think it's particularly good for processing. But but the the notion of what you want to do is better most of the time determined by the human mind. What are the principles like? OK, how should I raise my children? It's going to be a long time before a you're going to say it has a good enough judgment to do that. Who should I marry? All of those things. Maybe you can get the computer to help you. But if you just took data and do machine learning, it's not going to find it. If you were to then take one of my criteria for any of those questions and then, say, put them into an algorithm, you'd be a lot better off than if you took a I to do it. But by and large, the mind should be do used for inventing and those creative things. And then the computer should be used for processing because it could process a lot more information, a lot faster, a lot more accurately and a lot less emotionally. So any notion of thinking in the form of processing type thinking should be done by a computer. And anything that is in the notion of doing that other type of thinking should be operating with with the brain operating in a way where, you know, you can say, ah, that makes sense. You know, the process of reducing your understanding down to principles is kind of like the process, the the first one you mentioned, a type of algorithm where you're encoding your expertise\", metadata={'channel_id': 'UCSHZKyawb77ixDdsGog4iWA', 'chunk': 11.0, 'published': datetime.datetime(2019, 12, 2, 17, 12, 36), 'source': 'https://youtu.be/M95m2EFb7IQ', 'title': 'Ray Dalio: Principles, the Economic Machine, AI &amp; the Arc of Life | Lex Fridman Podcast #54', 'video_id': 'M95m2EFb7IQ'}),\n",
       "  Document(page_content=\"With Lending AI, you help already established companies grow their AI and machine learning efforts. How does a large company integrate machine learning into their efforts? AI is a general purpose technology, and I think it will transform every industry. Our community has already transformed to a large extent the software internet sector. Most software internet companies, they're outside the top, right? Five or six or three or four, already have reasonable machine learning capabilities or getting there. It's still room for improvement. But when I look outside the software internet sector, everything from manufacturing, agriculture, healthcare, logistics, transportation, there's so many opportunities that very few people are working on. So I think the next wave for AI is for us to also transform all of those other industries. There was a McKinsey study estimating $13 trillion of global economic growth. US GDP is $19 trillion, so 13 trillion is a big number, or PWC has been $16 trillion. So whatever number is, it's large. But the interesting thing to me was a lot of that impact would be outside the software internet sector. So we need more teams to work with these companies to help them adopt AI. And I think this is one of the things that'll make, help drive global economic growth and make humanity more powerful. And like you said, the impact is there. So what are the best industries, the biggest industries where AI can help, perhaps outside the software tech sector? Frankly, I think it's all of them. Some of the ones I'm spending a lot of time on are manufacturing, agriculture, looking into healthcare. For example, in manufacturing, we do a lot of work in visual inspection, where today there are people standing around using the eye, human eye, to check if this plastic pot or the smartphone or this thing has a scratch or a dent or something in it. We can use a camera to take a picture, use a algorithm, deep learning and other things to check if it's defective or not, and thus help factories improve yield and improve quality and improve throughput. It turns out the practical problems we run into are very different than the ones you might read about in most research papers. The data sets are really small, so we face small data problems. You know, the factories keep on changing the environment, so it works well on your test set, but guess what? You know, something changes in the factory. The lights go on or off. Recently, there was a factory in which a bird flew through the factory and pooped on something, and so that changed stuff. And so increasing our algorithmic robustness, so all the changes happen in the factory. I find that we run into a lot of practical problems that are not as widely discussed in academia, and it's really fun kind of being on the cutting edge, solving these problems before, you know, maybe before many people are even aware that there is a problem there. And that's such a fascinating space. You're absolutely right, but what is the first step that a company should take? It's just scary leap into this new world of going from the human eye inspecting to digitizing that process, having a camera, having an algorithm. What's the first step? Like, what's the early journey that you recommend that you see these companies taking? I published a document called the AI Transformation Playbook that's online and taught briefly in the AI for Everyone course on Coursera about the long-term journey that companies should take. But the first step is actually to start small. I've seen a lot more companies fail by starting too big than by starting too small. Take even Google. You know, most people don't realize how hard it was and how controversial it was in the early days. So when I started Google Brain, it was controversial. People thought deep learning, neural nets, tried it, didn't work. Why would you want to do deep learning? So my first internal customer within Google was the Google speech team, which is not the most lucrative project in Google, not the most important. It's not web search or advertising. But by starting small, my team helped the speech team build a more accurate speech recognition system. And this caused their peers, other teams, to start to have more faith in deep learning. My second internal customer was the Google Maps team, where we used computer vision to read house numbers from basic street view images to more accurately locate houses within Google Maps to improve the quality of the geodata. And it was only after those two successes that I then started a more serious conversation with the Google Ads team. And so there's a ripple effect that you showed that it works in these cases, and then it just propagates through the entire company, that this thing has a lot of value and use for us\", metadata={'channel_id': 'UCSHZKyawb77ixDdsGog4iWA', 'chunk': 19.0, 'published': datetime.datetime(2020, 2, 20, 17, 16, 1), 'source': 'https://youtu.be/0jspaMLxBig', 'title': 'Andrew Ng: Deep Learning, Education, and Real-World AI | Lex Fridman Podcast #73', 'video_id': '0jspaMLxBig'}),\n",
       "  Document(page_content=\"So machine learning is automation on steroids. If we take one task out of many tasks that are done in the factory, so the factory does lots of things. One task is visual inspection. If we automate that one task, it can be really valuable, but you may need to redesign a lot of other tasks around that one task. For example, say the machine learning algorithm says this is defective. What are you supposed to do? Do you throw it away? Do you get a human to double check? Do you want to rework it or fix it? So you need to redesign a lot of tasks around that thing you've now automated. So planning for the change management and making sure that the software you write is consistent with the new workflow. And you take the time to explain to people when these things happen. So I think what landing AI has become good at, and then I think we learned by making missteps and painful experiences. What we've become good at is working with our partners to think through all the things beyond just the machine learning model, running a Jupyter notebook, but to build the entire system, manage the change process and figure out how to deploy this in a way that has an actual impact. The processes that the large software tech companies use for deploying don't work for a lot of other scenarios. For example, when I was leading large speech teams, if the speech recognition system goes down, what happens? Well, allowance goes off, and then someone like me would say, hey, you 20 engineers, please fix this. Right? And then we'll get. But if you have a system go down in the factory, there are not 20 machine learning engineers sitting around, you can page a duty and have them fix it. So how do you deal with the maintenance or the dev ops or the ML ops or the other aspects of this? So these are concepts that I think landing AI and a few other teams are on the cutting edge of, but we don't even have systematic terminology yet to describe some of the stuff we do, because I think we're indenting it on the fly. So you mentioned some people are interested in discovering mathematical beauty and truth in the universe, and you're interested in having a big positive impact in the world. So let me ask- The two are not inconsistent. No, they're all together. I'm only half joking, because you're probably interested a little bit in both. But let me ask a romanticized question. So much of the work, your work and our discussion today has been on applied AI. Maybe you can even call narrow AI, where the goal is to create systems that automate some specific process that adds a lot of value to the world. But there's another branch of AI, starting with Alan Turing, that kind of dreams of creating human level or superhuman level intelligence. Is this something you dream of as well? Do you think we human beings will ever build a human level intelligence or superhuman level intelligence system? I would love to get to AGI, and I think humanity will, but whether it takes a hundred years or 500 or 5,000, I find hard to estimate. Do you have- So some folks have worries about the different trajectories that path would take, even existential threats of an AGI system. Do you have such concerns, whether in the short term or the long term? I do worry about the long-term fate of humanity. I do wonder as well. I do worry about overpopulation on the planet Mars, just not today. I think there will be a day when, maybe someday in the future, Mars will be polluted, there are all these children dying, and someone will look back at this video and say, Andrew, how was Andrew so heartless? He didn't care about all these children dying on the planet Mars. And I apologize to the future viewer. I do care about the children, but I just don't know how to productively work on that today. Your picture will be in the dictionary for the people who are ignorant about the overpopulation on Mars. Yes, so it's a long-term problem. Is there something in the short term we should be thinking about in terms of aligning the values of our AI systems with the values of us humans? Something that Stuart Russell and other folks are thinking about as this system develops more and more, we want to make sure that it represents the better angels of our nature, the ethics, the values of our society. If you take self-driving cars, the biggest problem with self-driving cars is not that there's some trolley dilemma, and you teach this, so how many times when you are driving your car, did you face this moral dilemma? Who do I crash into? So I think self-driving cars will run into that problem roughly as often as we do when we drive our cars\", metadata={'channel_id': 'UCSHZKyawb77ixDdsGog4iWA', 'chunk': 21.0, 'published': datetime.datetime(2020, 2, 20, 17, 16, 1), 'source': 'https://youtu.be/0jspaMLxBig', 'title': 'Andrew Ng: Deep Learning, Education, and Real-World AI | Lex Fridman Podcast #73', 'video_id': '0jspaMLxBig'})]}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac4934df",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
